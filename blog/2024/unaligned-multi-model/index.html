<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> In-Depth Look at the Multimodal Transformer for Unaligned Multimodal Language Sequences | Muhammad Farhan </title> <meta name="author" content="Muhammad Farhan"> <meta name="description" content="this is what included tabs in a post could look like"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://farhanzafrani.github.io/blog/2024/unaligned-multi-model/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Muhammad</span> Farhan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">In-Depth Look at the Multimodal Transformer for Unaligned Multimodal Language Sequences</h1> <p class="post-meta"> Created in May 01, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/multi-model-transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> multi-model-transformer</a>   ·   <a href="/blog/category/transformers"> <i class="fa-solid fa-tag fa-sm"></i> transformers,</a>   <a href="/blog/category/deep-learning"> <i class="fa-solid fa-tag fa-sm"></i> deep-learning,</a>   <a href="/blog/category/multi-model-transformers"> <i class="fa-solid fa-tag fa-sm"></i> multi-model-transformers</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h4"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#key-challenges-addressed-by-the-model">Key Challenges Addressed by the Model</a></li> <li class="toc-entry toc-h3"> <a href="#multimodal-transformer-architecture">Multimodal Transformer Architecture</a> <ul> <li class="toc-entry toc-h4"><a href="#1-feature-extraction">1. Feature Extraction</a></li> <li class="toc-entry toc-h4"><a href="#2-modality-specific-encoders">2. Modality-Specific Encoders</a></li> <li class="toc-entry toc-h4"><a href="#3-cross-modal-attention-mechanism">3. Cross-Modal Attention Mechanism</a></li> <li class="toc-entry toc-h4"><a href="#4-multimodal-fusion-layer">4. Multimodal Fusion Layer</a></li> <li class="toc-entry toc-h4"><a href="#5-final-prediction">5. Final Prediction</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#training-the-multimodal-transformer">Training the Multimodal Transformer</a></li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> <li class="toc-entry toc-h3"><a href="#references">References</a></li> </ul> </div> <hr> <div id="markdown-content"> <figure> <img src="/assets/img/unaligned_multi_model_transformer.png" alt="transformer" width="800"> <figcaption>Unaligned_multi_model_transformer.</figcaption> </figure> <h4 id="introduction"><strong>Introduction</strong></h4> <p>The paper <strong>“Multimodal Transformer for Unaligned Multimodal Language Sequences”</strong> introduces a powerful approach to tackle unaligned multimodal sequences by leveraging the Transformer architecture. This is crucial for tasks like video understanding, sentiment analysis, and language grounding where different modalities, such as text, speech, and visual data, are not synchronized in time. In this post, we’ll dive deep into the <strong>model architecture</strong> and explain how the <strong>Multimodal Transformer (MT)</strong> achieves its performance with detailed explanations and code snippets to bring the concept to life.</p> <hr> <h3 id="key-challenges-addressed-by-the-model"><strong>Key Challenges Addressed by the Model</strong></h3> <p>Before we dig into the architecture, let’s review the challenges the Multimodal Transformer aims to solve:</p> <ol> <li> <strong>Unaligned Modalities</strong>: Different modalities (e.g., text and video) may not be temporally aligned.</li> <li> <strong>Asynchronous Timing</strong>: The duration and timing of different modality streams can vary significantly.</li> <li> <strong>Cross-Modal Dependencies</strong>: Capturing meaningful relationships between modalities, even when they are temporally misaligned. <hr> <h3 id="multimodal-transformer-architecture"><strong>Multimodal Transformer Architecture</strong></h3> <p>The Multimodal Transformer (MT) is designed to handle input from multiple modalities and processes them simultaneously using a combination of <strong>self-attention</strong> and <strong>cross-modal attention</strong> mechanisms. The key components of the architecture are:</p> <h4 id="1-feature-extraction"><strong>1. Feature Extraction</strong></h4> <p>The first step is extracting features from each modality independently. These features are passed to modality-specific transformers for separate processing. Common feature extractors include:</p> <ul> <li> <strong>Text</strong>: Using pre-trained embeddings like BERT or Word2Vec.</li> <li> <strong>Audio</strong>: Using acoustic features like MFCC (Mel Frequency Cepstral Coefficients).</li> <li> <strong>Visual</strong>: Using CNN-based feature extractors for image or video frames.</li> </ul> </li> </ol> <p>For simplicity, here’s a basic Python pseudocode for feature extraction:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">BertModel</span>
<span class="kn">import</span> <span class="n">librosa</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torchvision.models</span> <span class="k">as</span> <span class="n">models</span>

<span class="c1"># Text Feature Extraction
</span><span class="n">text_model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">)</span>
<span class="n">text_inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">sample text</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">pt</span><span class="sh">'</span><span class="p">)</span>
<span class="n">text_features</span> <span class="o">=</span> <span class="nf">text_model</span><span class="p">(</span><span class="o">**</span><span class="n">text_inputs</span><span class="p">).</span><span class="n">last_hidden_state</span>

<span class="c1"># Audio Feature Extraction
</span><span class="n">audio</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">audio_file.wav</span><span class="sh">'</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16000</span><span class="p">)</span>
<span class="n">audio_features</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="nf">mfcc</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">)</span>

<span class="c1"># Visual Feature Extraction
</span><span class="n">cnn_model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">visual_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>  <span class="c1"># Simulating an image
</span><span class="n">visual_features</span> <span class="o">=</span> <span class="nf">cnn_model</span><span class="p">(</span><span class="n">visual_input</span><span class="p">)</span>
</code></pre></div></div> <p>Each extracted feature set is passed to its respective <strong>modality-specific encoder</strong>.</p> <hr> <h4 id="2-modality-specific-encoders"><strong>2. Modality-Specific Encoders</strong></h4> <figure> <img src="/assets/img/architecture.png" alt="architecture" width="800"> <figcaption>Image is taken from paper github repo.</figcaption> </figure> <p>Each modality (text, audio, video) is processed through its own transformer encoder, which consists of multiple layers of <strong>self-attention</strong> and <strong>feed-forward networks</strong>. These encoders learn the intra-modality relationships. The transformer encoder follows the traditional architecture:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">TransformerEncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="o">=</span><span class="n">nhead</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transformer_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer_encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="c1"># Example for Text
</span><span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">nhead</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">6</span>

<span class="n">text_encoder</span> <span class="o">=</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
<span class="n">text_encoded</span> <span class="o">=</span> <span class="nf">text_encoder</span><span class="p">(</span><span class="n">text_features</span><span class="p">)</span>
</code></pre></div></div> <p>Each modality’s encoder operates independently, generating <strong>modality-specific hidden representations</strong>.</p> <hr> <h4 id="3-cross-modal-attention-mechanism"><strong>3. Cross-Modal Attention Mechanism</strong></h4> <figure> <img src="/assets/img/cross_model.png" alt="transformer" width="800"> <figcaption>Image is taken from paper github repo.</figcaption> </figure> <p>One of the most innovative parts of this model is the <strong>Cross-Modal Attention Mechanism</strong>, which allows different modalities to attend to each other. This mechanism is crucial for capturing relationships between text, audio, and visual streams, even when they are temporally misaligned.</p> <p>Cross-modal attention works by learning relationships between the representations of different modalities. The model computes <strong>attention weights</strong> between all elements across modalities, allowing the information flow between asynchronous data streams.</p> <p>Here’s a simplified code snippet demonstrating the cross-modal attention:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">CrossModalAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CrossModalAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">modality_a</span><span class="p">,</span> <span class="n">modality_b</span><span class="p">):</span>
        <span class="c1"># Q, K, V for cross-modality interaction
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">modality_a</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">modality_b</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">modality_b</span><span class="p">)</span>

        <span class="c1"># Attention computation
</span>        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">cross_modal_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">cross_modal_output</span>

<span class="c1"># Example: Text attending to Video features
</span><span class="n">cross_modal_attention</span> <span class="o">=</span> <span class="nc">CrossModalAttention</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">cross_modal_output</span> <span class="o">=</span> <span class="nf">cross_modal_attention</span><span class="p">(</span><span class="n">text_encoded</span><span class="p">,</span> <span class="n">visual_features</span><span class="p">)</span>
</code></pre></div></div> <p>In this example, we make <strong>text</strong> attend to <strong>video features</strong>, enabling the model to understand the connection between the text and corresponding visual actions, even if they aren’t perfectly aligned.</p> <hr> <h4 id="4-multimodal-fusion-layer"><strong>4. Multimodal Fusion Layer</strong></h4> <p>Once cross-modal attention has been applied, the outputs from all modalities are combined into a unified representation. This <strong>fusion layer</strong> can be as simple as concatenation or more complex, using learned fusion techniques (e.g., weighted averaging or attention fusion). Here’s an example of multimodal fusion using concatenation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FusionLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">FusionLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text_features</span><span class="p">,</span> <span class="n">audio_features</span><span class="p">,</span> <span class="n">visual_features</span><span class="p">):</span>
        <span class="n">combined_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">text_features</span><span class="p">,</span> <span class="n">audio_features</span><span class="p">,</span> <span class="n">visual_features</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">fused_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">combined_features</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fused_output</span>

<span class="c1"># Fusion of text, audio, and video
</span><span class="n">fusion_layer</span> <span class="o">=</span> <span class="nc">FusionLayer</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">fused_representation</span> <span class="o">=</span> <span class="nf">fusion_layer</span><span class="p">(</span><span class="n">text_encoded</span><span class="p">,</span> <span class="n">audio_features</span><span class="p">,</span> <span class="n">visual_features</span><span class="p">)</span>
</code></pre></div></div> <p>In the example above, the output of each modality is concatenated, passed through a linear layer, and used to predict the final outcome (e.g., classification).</p> <hr> <h4 id="5-final-prediction"><strong>5. Final Prediction</strong></h4> <p>The final step involves predicting the output (e.g., classification, sentiment score, action detection) from the fused multimodal representation. A fully connected layer (or layers) is applied to produce the final output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PredictionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PredictionHead</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">fused_representation</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">fused_representation</span><span class="p">)</span>

<span class="c1"># Example: Sentiment Classification
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">prediction_head</span> <span class="o">=</span> <span class="nc">PredictionHead</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">final_output</span> <span class="o">=</span> <span class="nf">prediction_head</span><span class="p">(</span><span class="n">fused_representation</span><span class="p">)</span>
</code></pre></div></div> <hr> <h3 id="training-the-multimodal-transformer"><strong>Training the Multimodal Transformer</strong></h3> <p>The model is trained end-to-end using a <strong>contrastive loss</strong> that pushes similar representations of different modalities closer and dissimilar ones apart. The <strong>cross-modal contrastive loss</strong> helps in learning better interactions across asynchronous modalities. Here’s a brief code snippet to demonstrate how contrastive loss could be implemented:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ContrastiveLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ContrastiveLoss</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">pairwise_distance</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span>
               <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">label</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">margin</span> <span class="o">-</span> <span class="n">distance</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

<span class="c1"># Example: Compute contrastive loss between audio and visual representations
</span><span class="n">contrastive_loss</span> <span class="o">=</span> <span class="nc">ContrastiveLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">contrastive_loss</span><span class="p">(</span><span class="n">audio_features</span><span class="p">,</span> <span class="n">visual_features</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</code></pre></div></div> <p>This loss function helps the model learn meaningful relationships across modalities and improves its ability to generalize to unaligned sequences.</p> <hr> <h3 id="conclusion"><strong>Conclusion</strong></h3> <p>The <strong>Multimodal Transformer for Unaligned Multimodal Language Sequences</strong> tackles a challenging problem in machine learning: integrating asynchronous and unaligned data from multiple modalities. By leveraging a combination of <strong>self-attention</strong>, <strong>cross-modal attention</strong>, and <strong>contrastive learning</strong>, this model successfully captures dependencies between different modalities, even when they’re not perfectly synchronized. The architecture, consisting of modality-specific transformers, cross-modal attention, and fusion layers, is both elegant and powerful, making it applicable across various tasks involving complex multimodal data. ***</p> <h3 id="references"><strong>References</strong></h3> <p><a href="https://github.com/yaohungt/Multimodal-Transformer" rel="external nofollow noopener" target="_blank">Paper Code</a></p> <p><a href="https://arxiv.org/pdf/1906.00295" rel="external nofollow noopener" target="_blank">Official Paper</a></p> <p><a href="https://www.youtube.com/watch?v=JTbX4OyUF-c" rel="external nofollow noopener" target="_blank">YouTube_video</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/einsum/">Use of `np.einsum()`</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/self-attension-in-transformers/">Understanding Self-Attention in Transformers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/typograms/">a post with typograms</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Muhammad Farhan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-people",title:"people",description:"co-workers",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"post-use-of-np-einsum",title:"Use of `np.einsum()`",description:"Use of `np.einsum` to manipulate multi dimensional arrays",section:"Posts",handler:()=>{window.location.href="/blog/2024/einsum/"}},{id:"post-understanding-self-attention-in-transformers",title:"Understanding Self-Attention in Transformers",description:"understanding self-attension mechanism in transformers in details",section:"Posts",handler:()=>{window.location.href="/blog/2024/self-attension-in-transformers/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-in-depth-look-at-the-multimodal-transformer-for-unaligned-multimodal-language-sequences",title:"In-Depth Look at the Multimodal Transformer for Unaligned Multimodal Language Sequences",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/unaligned-multi-model/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"news-use-of-einsum-einstain-summation-for-handling-multi-dimensional-arrays-sparkles",title:'Use of `einsum` (Einstain Summation) for handling multi dimensional arrays.<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-self-attention-mechanism-with-transformers-sparkles-smile",title:'Self Attention mechanism with transformers! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-coccidiosis-disease-prediction-using-chicken-fecals",title:"Coccidiosis Disease Prediction Using Chicken Fecals",description:"It's a complete end to end ML project that include MLOps as well.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-multimodel-vision-language-model-from-scratch",title:"Multimodel Vision Language Model from scratch",description:"Implementing Multimodel Vision Language Model for handling both image and text data at a time using Pytorch",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-transformer-from-scratch",title:"Transformer from Scratch",description:"Transformer implementation from scratch according to `Attention all you need` paper. For development, I will use pytorch.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-yolov3-from-scratch",title:"YOLOv3-from-scratch",description:"implementing VOLOv3 from scratch using python and pytorch.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-spotify-database-development-using-fastapi-and-sqlite",title:"Spotify Database Development using FastAPI and SQlite",description:"This project portrays how to prepare the data from a single source to create a database and then perform CRUD operations on it.",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%66%61%72%68%61%6E%7A%61%66%72%61%6E%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/farhanzafrani","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/farhan-zafrani","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/https://www.kaggle.com/farhanzafrani","_blank")}},{id:"socials-facebook",title:"Facebook",section:"Socials",handler:()=>{window.open("https://facebook.com/https://www.facebook.com/profile.php?id=100012963545472","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>