<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Self-Attention in Transformers | Muhammad Farhan </title> <meta name="author" content="Muhammad Farhan"> <meta name="description" content="understanding self-attension mechanism in transformers in details"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://farhanzafrani.github.io/blog/2024/self-attension-in-transformers/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Muhammad</span> Farhan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Self-Attention in Transformers</h1> <p class="post-meta"> Created in September 01, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/self-attention"> <i class="fa-solid fa-hashtag fa-sm"></i> self_attention</a>   ·   <a href="/blog/category/transformers"> <i class="fa-solid fa-tag fa-sm"></i> transformers</a>   <a href="/blog/category/"> <i class="fa-solid fa-tag fa-sm"></i> ,</a>   <a href="/blog/category/deeplearnings"> <i class="fa-solid fa-tag fa-sm"></i> deeplearnings,</a>   <a href="/blog/category/sequence"> <i class="fa-solid fa-tag fa-sm"></i> sequence</a>   <a href="/blog/category/models"> <i class="fa-solid fa-tag fa-sm"></i> models</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#introduction-to-word-embeddings">Introduction to Word Embeddings</a> <ul> <li class="toc-entry toc-h3"><a href="#example-word2vec-embeddings">Example: Word2Vec Embeddings</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#limitations-of-traditional-word-embeddings">Limitations of Traditional Word Embeddings</a> <ul> <li class="toc-entry toc-h3"><a href="#21-contextual-insensitivity">2.1. Contextual Insensitivity</a></li> <li class="toc-entry toc-h3"><a href="#22-inefficient-capture-of-long-range-dependencies">2.2. Inefficient Capture of Long-Range Dependencies</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#3-what-is-self-attention">3. What is Self-Attention?</a></li> <li class="toc-entry toc-h2"> <a href="#how-does-self-attention-work">How Does Self-Attention Work?</a> <ul> <li class="toc-entry toc-h3"><a href="#41-the-concept">4.1. The Concept</a></li> <li class="toc-entry toc-h3"><a href="#42-mathematical-formulation">4.2. Mathematical Formulation</a></li> <li class="toc-entry toc-h3"><a href="#43-scaled-dot-product-attention">4.3. Scaled Dot-Product Attention</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#implementing-self-attention-in-code">Implementing Self-Attention in Code</a> <ul> <li class="toc-entry toc-h3"><a href="#51-single-head-self-attention">5.1. Single-Head Self-Attention</a></li> <li class="toc-entry toc-h3"><a href="#52-multi-head-self-attention">5.2. Multi-Head Self-Attention</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#benefits-of-self-attention">Benefits of Self-Attention</a></li> <li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li> <li class="toc-entry toc-h2"><a href="#references">References</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>============================================</p> <p><img src="/assets/img/transformer_with_pytorch.png" alt="transformer" width="800"></p> <p>Self-Attention is a fundamental mechanism in modern Natural Language Processing (NLP) models, especially in Transformer architectures. It allows models to weigh the importance of different words in a sequence when encoding a particular word, enabling the capture of contextual relationships effectively.</p> <p>This guide provides a detailed understanding of self-attention, its necessity, and how it works, along with illustrative examples and code snippets.</p> <h2 id="introduction-to-word-embeddings">Introduction to Word Embeddings</h2> <hr> <p><strong>Word Embeddings</strong> are numerical representations of words in a continuous vector space where semantically similar words are mapped closely together. They have been instrumental in capturing the meaning and context of words in various NLP tasks.</p> <h3 id="example-word2vec-embeddings">Example: Word2Vec Embeddings</h3> <p>Word2Vec is a popular technique for generating word embeddings by training neural networks on large corpora.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of obtaining word embeddings using gensim
</span><span class="kn">from</span> <span class="n">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="sh">"</span><span class="s">king</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">queen</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">man</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">woman</span><span class="sh">"</span><span class="p">],</span>
             <span class="p">[</span><span class="sh">"</span><span class="s">apple</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fruit</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">banana</span><span class="sh">"</span><span class="p">]]</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">vector</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="sh">'</span><span class="s">king</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Retrieves the embedding for 'king'
</span></code></pre></div></div> <p><strong>Properties of Word Embeddings:</strong></p> <ul> <li>Capture semantic relationships (e.g., <code class="language-plaintext highlighter-rouge">king</code> - <code class="language-plaintext highlighter-rouge">man</code> + <code class="language-plaintext highlighter-rouge">woman</code> ≈ <code class="language-plaintext highlighter-rouge">queen</code>).</li> <li>Represent words in fixed-size dense vectors.</li> </ul> <hr> <h2 id="limitations-of-traditional-word-embeddings">Limitations of Traditional Word Embeddings</h2> <hr> <p>Despite their effectiveness, traditional word embeddings have several limitations:</p> <h3 id="21-contextual-insensitivity">2.1. Contextual Insensitivity</h3> <ul> <li> <strong>Static Representations:</strong> Each word has a single embedding regardless of context.</li> <li> <strong>Ambiguity Handling:</strong> Words with multiple meanings (polysemy) are not adequately represented.</li> </ul> <p><strong>Example:</strong></p> <p>Consider the word <em>“apple”</em> in different contexts:</p> <ol> <li> <em>“I ate an apple for breakfast.”</em> (referring to the fruit)</li> <li> <em>“Apple released a new iPhone model.”</em> (referring to the tech company)</li> </ol> <p>Traditional embeddings assign the same vector to <em>“apple”</em> in both contexts, failing to capture the different meanings.</p> <h3 id="22-inefficient-capture-of-long-range-dependencies">2.2. Inefficient Capture of Long-Range Dependencies</h3> <ul> <li>Difficulty in modeling relationships between distant words in a sequence.</li> <li>Challenges in understanding complex sentence structures.</li> </ul> <p><strong>Need for Contextual Representations:</strong> To overcome these limitations, models require mechanisms that can dynamically adjust word representations based on context. This is where <strong>Self-Attention</strong> comes into play.</p> <hr> <h2 id="3-what-is-self-attention">3. What is Self-Attention?</h2> <p><strong>Self-Attention</strong> is a mechanism that allows a model to focus on different parts of the input sequence when encoding a particular element. It computes a representation of each word by considering its relationship with other words in the sequence.</p> <p><strong>Key Characteristics:</strong></p> <ul> <li> <strong>Contextual Understanding:</strong> Adjusts word representations based on surrounding words.</li> <li> <strong>Parallel Computation:</strong> Enables efficient processing of sequences.</li> <li> <strong>Flexibility:</strong> Captures both short and long-range dependencies effectively.</li> </ul> <p><strong>High-Level Idea:</strong> For each word in a sequence, self-attention computes attention weights indicating the importance of other words in understanding the current word.</p> <p><strong>Visual Representation:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Sequence: <span class="o">[</span>The, <span class="nb">cat</span>, sat, on, the, mat]

Self-Attention computes:
- For <span class="s1">'cat'</span>, higher attention to <span class="s1">'sat'</span> and <span class="s1">'mat'</span>
- For <span class="s1">'mat'</span>, higher attention to <span class="s1">'on'</span> and <span class="s1">'cat'</span>
</code></pre></div></div> <hr> <h2 id="how-does-self-attention-work">How Does Self-Attention Work?</h2> <hr> <h3 id="41-the-concept">4.1. The Concept</h3> <p>Self-Attention involves three main components for each word/token in the sequence:</p> <ol> <li> <strong>Query (Q):</strong> Represents the current word we’re focusing on.</li> <li> <strong>Key (K):</strong> Represents all other words in the sequence.</li> <li> <strong>Value (V):</strong> Contains information to be aggregated based on attention weights.</li> </ol> <p><strong>Process Overview:</strong></p> <ul> <li>Compute dot products between the Query and all Keys to get attention scores.</li> <li>Apply softmax to obtain attention weights.</li> <li>Multiply attention weights with the corresponding Values.</li> <li>Sum the results to get the new representation for the current word.</li> </ul> <h3 id="42-mathematical-formulation">4.2. Mathematical Formulation</h3> <p>Given an input sequence of embeddings X∈Rn×dX \in \mathbb{R}^{n \times d}X∈Rn×d:</p> <ol> <li> <p><strong>Linear Transformations:</strong></p> <ul> <li>Q=XWQQ = XW^QQ=XWQ</li> <li>K=XWKK = XW^KK=XWK</li> <li>V=XWVV = XW^VV=XWV</li> <li>Where: <ul> <li>WQ,WK,WV∈Rd×dkW^Q, W^K, W^V \in \mathbb{R}^{d \times d_k}WQ,WK,WV∈Rd×dk​ are learnable weight matrices.</li> <li>nnn is the sequence length.</li> <li>ddd is the embedding dimension.</li> <li>dkd_kdk​ is the dimension of queries and keys.</li> </ul> </li> </ul> </li> <li> <p><strong>Attention Scores:</strong></p> <ul> <li>Attention(Q,K,V)=softmax(QKTdk)V\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) VAttention(Q,K,V)=softmax(dk​​QKT​)V</li> </ul> </li> </ol> <p><strong>Explanation:</strong></p> <ul> <li> <strong>Dot Product (QK^T):</strong> Measures similarity between queries and keys.</li> <li> <strong>Scaling (dk\sqrt{d_k}dk​​):</strong> Prevents large dot product values leading to small gradients.</li> <li> <strong>Softmax:</strong> Converts scores to probabilities representing attention weights.</li> <li> <strong>Weighted Sum (V):</strong> Aggregates values based on attention weights to form contextualized embeddings.</li> </ul> <h3 id="43-scaled-dot-product-attention">4.3. Scaled Dot-Product Attention</h3> <p><strong>Formula:</strong></p> <p>Attention(Q,K,V)=softmax(QKTdk)V\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) VAttention(Q,K,V)=softmax(dk​​QKT​)V</p> <p><strong>Components Explained:</strong></p> <ul> <li> <strong>Q (Query):</strong> Captures what we’re searching for.</li> <li> <strong>K (Key):</strong> Captures what information we have.</li> <li> <strong>V (Value):</strong> Contains the actual information.</li> </ul> <p><strong>Why Scale by dk\sqrt{d_k}dk​​?</strong></p> <ul> <li>To counteract the effect of high-dimensional dot products, which can result in extremely large values, leading to vanishing gradients during training.</li> </ul> <p><strong>Illustrative Diagram:</strong></p> <div class="language-css highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span><span class="nt">Input</span> <span class="nt">Embeddings</span><span class="o">]</span> <span class="nt">--</span><span class="o">&gt;</span> <span class="o">[</span><span class="nt">Q</span><span class="o">,</span> <span class="nt">K</span><span class="o">,</span> <span class="nt">V</span> <span class="nt">Linear</span> <span class="nt">Layers</span><span class="o">]</span> <span class="nt">--</span><span class="o">&gt;</span> <span class="o">[</span><span class="nt">Compute</span> <span class="nt">Attention</span> <span class="nt">Scores</span><span class="o">]</span> <span class="nt">--</span><span class="o">&gt;</span> <span class="o">[</span><span class="nt">Weighted</span> <span class="nt">Sum</span> <span class="nt">with</span> <span class="nt">V</span><span class="o">]</span> <span class="nt">--</span><span class="o">&gt;</span> <span class="o">[</span><span class="nt">Output</span> <span class="nt">Embeddings</span><span class="o">]</span>
</code></pre></div></div> <hr> <h2 id="implementing-self-attention-in-code">Implementing Self-Attention in Code</h2> <hr> <p>We’ll implement both single-head and multi-head self-attention mechanisms using PyTorch.</p> <h3 id="51-single-head-self-attention">5.1. Single-Head Self-Attention</h3> <p><strong>PyTorch Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_size</span> <span class="o">=</span> <span class="n">embed_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span>   <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: [batch_size, seq_length, embed_size]
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># [batch_size, seq_length, seq_length]
</span>        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_size</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># [batch_size, seq_length, embed_size]
</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p><strong>Usage Example:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
<span class="n">self_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">embed_size</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Output: torch.Size([2, 5, 64])`
</span></code></pre></div></div> <p><strong>Explanation:</strong></p> <ul> <li> <strong>Input:</strong> Batch of sequences with embeddings.</li> <li> <strong>Output:</strong> Contextualized embeddings where each position considers other positions in the sequence.</li> </ul> <h3 id="52-multi-head-self-attention">5.2. Multi-Head Self-Attention</h3> <p><strong>Why Multi-Head?</strong></p> <ul> <li>Allows the model to jointly attend to information from different representation subspaces at different positions.</li> </ul> <p><strong>PyTorch Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadSelfAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">embed_size</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">Embedding size must be divisible by num_heads</span><span class="sh">"</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embed_size</span> <span class="o">=</span> <span class="n">embed_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_size</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span>   <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">N</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Reshape for multi-head
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">embed_size</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p><strong>Usage Example:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
<span class="n">multi_head_attention</span> <span class="o">=</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">multi_head_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Output: torch.Size([2, 5, 64])
</span></code></pre></div></div> <p><strong>Explanation:</strong></p> <ul> <li> <strong>Splitting into Heads:</strong> The embedding is divided into multiple heads, allowing the model to focus on different parts of the input.</li> <li> <strong>Parallel Attention:</strong> Each head performs self-attention in parallel.</li> <li> <strong>Concatenation and Projection:</strong> The outputs from all heads are concatenated and passed through a final linear layer.</li> </ul> <hr> <h2 id="benefits-of-self-attention">Benefits of Self-Attention</h2> <hr> <ul> <li> <strong>Contextual Representations:</strong> Captures the context of words dynamically, handling polysemy effectively.</li> <li> <strong>Parallelism:</strong> Unlike recurrent models, self-attention allows parallel computation over sequence positions, leading to faster training.</li> <li> <strong>Long-Range Dependencies:</strong> Efficiently models relationships between distant words in a sequence.</li> <li> <strong>Scalability:</strong> Performs well on long sequences without significant computational overhead.</li> <li> <strong>Versatility:</strong> Applicable beyond NLP, including computer vision and speech processing tasks.</li> </ul> <p><strong>Comparison with Traditional Models:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>RNNs/LSTMs</th> <th>Self-Attention</th> </tr> </thead> <tbody> <tr> <td>Computation Parallelism</td> <td>Sequential</td> <td>Parallel</td> </tr> <tr> <td>Long-Range Dependencies</td> <td>Challenging to capture</td> <td>Efficiently captured</td> </tr> <tr> <td>Contextual Understanding</td> <td>Limited by sequence length</td> <td>Comprehensive across entire sequence</td> </tr> <tr> <td>Computational Efficiency</td> <td>Less efficient for long sequences</td> <td>More efficient and scalable</td> </tr> </tbody> </table> <hr> <h2 id="conclusion">Conclusion</h2> <hr> <p>Self-Attention has revolutionized the field of NLP by providing a powerful mechanism to capture contextual information effectively. It addresses the limitations of traditional word embeddings and recurrent models, enabling the development of sophisticated models like Transformers that have set new benchmarks across various tasks.</p> <p>Understanding self-attention is crucial for leveraging modern NLP architectures and for further innovation in the field.</p> <hr> <h2 id="references">References</h2> <hr> <ol> <li>Vaswani, A., et al. (2017). <a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank"><strong>“Attention is All You Need”</strong></a>. <em>Advances in Neural Information Processing Systems</em>.</li> <li>Devlin, J., et al. (2018). <a href="https://arxiv.org/abs/1810.04805" rel="external nofollow noopener" target="_blank"><strong>“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”</strong></a>.</li> <li> <strong>“The Illustrated Transformer”</strong> by Jay Alammar.</li> <li> <strong>PyTorch Documentation</strong>.</li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/einsum/">Use of `np.einsum()`</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/unaligned-multi-model/">In-Depth Look at the Multimodal Transformer for Unaligned Multimodal Language Sequences</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/typograms/">a post with typograms</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Muhammad Farhan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-people",title:"people",description:"members of the lab or group",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"post-use-of-np-einsum",title:"Use of `np.einsum()`",description:"Use of `np.einsum` to manipulate multi dimensional arrays",section:"Posts",handler:()=>{window.location.href="/blog/2024/einsum/"}},{id:"post-understanding-self-attention-in-transformers",title:"Understanding Self-Attention in Transformers",description:"understanding self-attension mechanism in transformers in details",section:"Posts",handler:()=>{window.location.href="/blog/2024/self-attension-in-transformers/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-in-depth-look-at-the-multimodal-transformer-for-unaligned-multimodal-language-sequences",title:"In-Depth Look at the Multimodal Transformer for Unaligned Multimodal Language Sequences",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/unaligned-multi-model/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"news-use-of-einsum-einstain-summation-for-handling-multi-dimensional-arrays-sparkles",title:'Use of `einsum` (Einstain Summation) for handling multi dimensional arrays.<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">',description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-self-attention-mechanism-with-transformers-sparkles-smile",title:'Self Attention mechanism with transformers! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-coccidiosis-disease-prediction-using-chicken-fecals",title:"Coccidiosis Disease Prediction Using Chicken Fecals",description:"It's a complete end to end ML project that include MLOps as well.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-multimodel-vision-language-model-from-scratch",title:"Multimodel Vision Language Model from scratch",description:"Implementing Multimodel Vision Language Model for handling both image and text data at a time using Pytorch",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-transformer-from-scratch",title:"Transformer from Scratch",description:"Transformer implementation from scratch according to `Attention all you need` paper. For development, I will use pytorch.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-yolov3-from-scratch",title:"YOLOv3-from-scratch",description:"implementing VOLOv3 from scratch using python and pytorch.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-spotify-database-development-using-fastapi-and-sqlite",title:"Spotify Database Development using FastAPI and SQlite",description:"This project portrays how to prepare the data from a single source to create a database and then perform CRUD operations on it.",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%66%61%72%68%61%6E%7A%61%66%72%61%6E%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/farhanzafrani","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/farhan-zafrani","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/https://www.kaggle.com/farhanzafrani","_blank")}},{id:"socials-facebook",title:"Facebook",section:"Socials",handler:()=>{window.open("https://facebook.com/https://www.facebook.com/profile.php?id=100012963545472","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>