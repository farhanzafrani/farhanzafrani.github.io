<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://farhanzafrani.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://farhanzafrani.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-24T17:42:32+00:00</updated><id>https://farhanzafrani.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Use of `np.einsum()`</title><link href="https://farhanzafrani.github.io/blog/2024/einsum/" rel="alternate" type="text/html" title="Use of `np.einsum()`"/><published>2024-09-17T00:00:00+00:00</published><updated>2024-09-17T00:00:00+00:00</updated><id>https://farhanzafrani.github.io/blog/2024/einsum</id><content type="html" xml:base="https://farhanzafrani.github.io/blog/2024/einsum/"><![CDATA[<p><code class="language-plaintext highlighter-rouge">np.einsum()</code> is a powerful function in NumPy that performs Einstein summation, which allows for flexible manipulation of multi-dimensional arrays (tensors) using summation notation. It can handle various operations like matrix multiplication, element-wise operations, and tensor contractions in a very efficient way.</p> <h3 id="syntax">Syntax:</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="n">subscripts</span><span class="p">,</span> <span class="o">*</span><span class="n">operands</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">subscripts</code>: A string representing the Einstein summation convention.</li> <li><code class="language-plaintext highlighter-rouge">*operands</code>: The arrays (tensors) on which the operation is performed.</li> <li><code class="language-plaintext highlighter-rouge">**kwargs</code>: Optional arguments like <code class="language-plaintext highlighter-rouge">optimize</code>, which can be used to improve performance.</li> </ul> <h3 id="einstein-summation-convention"><strong>Einstein Summation Convention</strong></h3> <p>The Einstein summation convention is a notational shorthand where repeated indices are implicitly summed over. For example:</p> <ul> <li><code class="language-plaintext highlighter-rouge">ij,jk-&gt;ik</code> denotes a matrix multiplication between two 2D arrays.</li> <li><code class="language-plaintext highlighter-rouge">ii-&gt;i</code> sums the diagonal elements of a matrix.</li> </ul> <h3 id="common-examples">Common Examples</h3> <ol> <li> <p><strong>Matrix Multiplication</strong> (<code class="language-plaintext highlighter-rouge">np.dot</code> or <code class="language-plaintext highlighter-rouge">np.matmul</code>)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="c1"># Matrix multiplication
</span><span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">ij,jk-&gt;ik</span><span class="sh">'</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div> </div> <p><strong>Explanation</strong>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">ij</code>: Refers to the indices of the matrix <code class="language-plaintext highlighter-rouge">A</code>.</li> <li><code class="language-plaintext highlighter-rouge">jk</code>: Refers to the indices of the matrix <code class="language-plaintext highlighter-rouge">B</code>.</li> <li><code class="language-plaintext highlighter-rouge">ik</code>: The result is the matrix multiplication of <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>.</li> </ul> </li> <li> <p><strong>Sum over an axis</strong> (similar to <code class="language-plaintext highlighter-rouge">np.sum</code>)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="c1"># Sum over all elements (similar to np.sum)
</span><span class="n">total_sum</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">ij-&gt;</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_array</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">total_sum</span><span class="p">)</span>
</code></pre></div> </div> <p><strong>Explanation</strong>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">ij-&gt;</code>: This notation sums over all indices of the matrix <code class="language-plaintext highlighter-rouge">input_array</code>, returning the total sum.</li> </ul> </li> <li> <p><strong>Trace of a Matrix</strong> (sum of diagonal elements)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="c1"># Trace (sum of diagonal elements)
</span><span class="n">trace</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">ii-&gt;</span><span class="sh">'</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div> </div> <p><strong>Explanation</strong>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">ii-&gt;</code>: This notation picks the diagonal elements of the matrix <code class="language-plaintext highlighter-rouge">input_array</code> and sums them.</li> </ul> </li> <li> <p><strong>Element-wise multiplication</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">second_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="c1"># Element-wise multiplication
</span><span class="n">element_wise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">ij,ij-&gt;ij</span><span class="sh">'</span><span class="p">,</span> <span class="n">first_array</span><span class="p">,</span> <span class="n">second_array</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">element_wise</span><span class="p">)</span>
</code></pre></div> </div> <p><strong>Explanation</strong>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">ij,ij-&gt;ij</code>: The indices <code class="language-plaintext highlighter-rouge">ij</code> are the same for both arrays <code class="language-plaintext highlighter-rouge">first_array</code> and <code class="language-plaintext highlighter-rouge">second_array</code>, resulting in element-wise multiplication.</li> </ul> </li> <li><strong>Dot product of vectors</strong> (<code class="language-plaintext highlighter-rouge">np.dot</code>) <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="c1"># Dot product
</span><span class="n">dot_product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">i,i-&gt;</span><span class="sh">'</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dot_product</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p><strong>Tensor contraction</strong> (generalized summation over axes)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">second_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># Contract tensor G and matrix H
</span><span class="n">tensor_contraction</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">ijk,jl-&gt;ikl</span><span class="sh">'</span><span class="p">,</span> <span class="n">first_array</span><span class="p">,</span> <span class="n">second_array</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">tensor_contraction</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div> </div> <p><strong>Explanation</strong>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">ijk,jl-&gt;ikl</code>: Summation is performed over the common axis <code class="language-plaintext highlighter-rouge">j</code>, resulting in a contraction of the tensor. <h3 id="advantages-of-npeinsum"><strong>Advantages of <code class="language-plaintext highlighter-rouge">np.einsum</code></strong></h3> </li> </ul> </li> </ol> <ul> <li><strong>Flexibility</strong>: You can perform many types of operations in a single function call.</li> <li><strong>Efficiency</strong>: It can be faster than separate functions like <code class="language-plaintext highlighter-rouge">np.dot</code>, <code class="language-plaintext highlighter-rouge">np.sum</code>, etc., especially when you have complex operations to perform.</li> </ul>]]></content><author><name></name></author><category term="mathematics,"/><category term="numpy"/><category term="Numpy"/><category term="Multi-dimensional-arrays"/><category term="Data-Analysis"/><summary type="html"><![CDATA[Use of `np.einsum` to manipulate multi dimensional arrays]]></summary></entry><entry><title type="html">Understanding Self-Attention in Transformers</title><link href="https://farhanzafrani.github.io/blog/2024/self-attension-in-transformers/" rel="alternate" type="text/html" title="Understanding Self-Attention in Transformers"/><published>2024-09-01T00:00:00+00:00</published><updated>2024-09-01T00:00:00+00:00</updated><id>https://farhanzafrani.github.io/blog/2024/self-attension-in-transformers</id><content type="html" xml:base="https://farhanzafrani.github.io/blog/2024/self-attension-in-transformers/"><![CDATA[<p>============================================</p> <p><img src="../assets/img/transformer-with-pytorch.png" alt="transformer"/></p> <p>Self-Attention is a fundamental mechanism in modern Natural Language Processing (NLP) models, especially in Transformer architectures. It allows models to weigh the importance of different words in a sequence when encoding a particular word, enabling the capture of contextual relationships effectively.</p> <p>This guide provides a detailed understanding of self-attention, its necessity, and how it works, along with illustrative examples and code snippets.</p> <h2 id="introduction-to-word-embeddings">Introduction to Word Embeddings</h2> <hr/> <p><strong>Word Embeddings</strong> are numerical representations of words in a continuous vector space where semantically similar words are mapped closely together. They have been instrumental in capturing the meaning and context of words in various NLP tasks.</p> <h3 id="example-word2vec-embeddings">Example: Word2Vec Embeddings</h3> <p>Word2Vec is a popular technique for generating word embeddings by training neural networks on large corpora.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of obtaining word embeddings using gensim
</span><span class="kn">from</span> <span class="n">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="sh">"</span><span class="s">king</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">queen</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">man</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">woman</span><span class="sh">"</span><span class="p">],</span>
             <span class="p">[</span><span class="sh">"</span><span class="s">apple</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fruit</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">banana</span><span class="sh">"</span><span class="p">]]</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">vector</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="sh">'</span><span class="s">king</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Retrieves the embedding for 'king'
</span></code></pre></div></div> <p><strong>Properties of Word Embeddings:</strong></p> <ul> <li>Capture semantic relationships (e.g., <code class="language-plaintext highlighter-rouge">king</code> - <code class="language-plaintext highlighter-rouge">man</code> + <code class="language-plaintext highlighter-rouge">woman</code> ≈ <code class="language-plaintext highlighter-rouge">queen</code>).</li> <li>Represent words in fixed-size dense vectors.</li> </ul> <hr/> <h2 id="limitations-of-traditional-word-embeddings">Limitations of Traditional Word Embeddings</h2> <hr/> <p>Despite their effectiveness, traditional word embeddings have several limitations:</p> <h3 id="21-contextual-insensitivity">2.1. Contextual Insensitivity</h3> <ul> <li><strong>Static Representations:</strong> Each word has a single embedding regardless of context.</li> <li><strong>Ambiguity Handling:</strong> Words with multiple meanings (polysemy) are not adequately represented.</li> </ul> <p><strong>Example:</strong></p> <p>Consider the word <em>“apple”</em> in different contexts:</p> <ol> <li><em>“I ate an apple for breakfast.”</em> (referring to the fruit)</li> <li><em>“Apple released a new iPhone model.”</em> (referring to the tech company)</li> </ol> <p>Traditional embeddings assign the same vector to <em>“apple”</em> in both contexts, failing to capture the different meanings.</p> <h3 id="22-inefficient-capture-of-long-range-dependencies">2.2. Inefficient Capture of Long-Range Dependencies</h3> <ul> <li>Difficulty in modeling relationships between distant words in a sequence.</li> <li>Challenges in understanding complex sentence structures.</li> </ul> <p><strong>Need for Contextual Representations:</strong> To overcome these limitations, models require mechanisms that can dynamically adjust word representations based on context. This is where <strong>Self-Attention</strong> comes into play.</p> <hr/> <h2 id="3-what-is-self-attention">3. What is Self-Attention?</h2> <p><strong>Self-Attention</strong> is a mechanism that allows a model to focus on different parts of the input sequence when encoding a particular element. It computes a representation of each word by considering its relationship with other words in the sequence.</p> <p><strong>Key Characteristics:</strong></p> <ul> <li><strong>Contextual Understanding:</strong> Adjusts word representations based on surrounding words.</li> <li><strong>Parallel Computation:</strong> Enables efficient processing of sequences.</li> <li><strong>Flexibility:</strong> Captures both short and long-range dependencies effectively.</li> </ul> <p><strong>High-Level Idea:</strong> For each word in a sequence, self-attention computes attention weights indicating the importance of other words in understanding the current word.</p> <p><strong>Visual Representation:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Sequence: <span class="o">[</span>The, <span class="nb">cat</span>, sat, on, the, mat]

Self-Attention computes:
- For <span class="s1">'cat'</span>, higher attention to <span class="s1">'sat'</span> and <span class="s1">'mat'</span>
- For <span class="s1">'mat'</span>, higher attention to <span class="s1">'on'</span> and <span class="s1">'cat'</span>
</code></pre></div></div> <hr/> <h2 id="how-does-self-attention-work">How Does Self-Attention Work?</h2> <hr/> <h3 id="41-the-concept">4.1. The Concept</h3> <p>Self-Attention involves three main components for each word/token in the sequence:</p> <ol> <li><strong>Query (Q):</strong> Represents the current word we’re focusing on.</li> <li><strong>Key (K):</strong> Represents all other words in the sequence.</li> <li><strong>Value (V):</strong> Contains information to be aggregated based on attention weights.</li> </ol> <p><strong>Process Overview:</strong></p> <ul> <li>Compute dot products between the Query and all Keys to get attention scores.</li> <li>Apply softmax to obtain attention weights.</li> <li>Multiply attention weights with the corresponding Values.</li> <li>Sum the results to get the new representation for the current word.</li> </ul> <h3 id="42-mathematical-formulation">4.2. Mathematical Formulation</h3> <p>Given an input sequence of embeddings X∈Rn×dX \in \mathbb{R}^{n \times d}X∈Rn×d:</p> <ol> <li> <p><strong>Linear Transformations:</strong></p> <ul> <li>Q=XWQQ = XW^QQ=XWQ</li> <li>K=XWKK = XW^KK=XWK</li> <li>V=XWVV = XW^VV=XWV</li> <li>Where: <ul> <li>WQ,WK,WV∈Rd×dkW^Q, W^K, W^V \in \mathbb{R}^{d \times d_k}WQ,WK,WV∈Rd×dk​ are learnable weight matrices.</li> <li>nnn is the sequence length.</li> <li>ddd is the embedding dimension.</li> <li>dkd_kdk​ is the dimension of queries and keys.</li> </ul> </li> </ul> </li> <li> <p><strong>Attention Scores:</strong></p> <ul> <li>Attention(Q,K,V)=softmax(QKTdk)V\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) VAttention(Q,K,V)=softmax(dk​​QKT​)V</li> </ul> </li> </ol> <p><strong>Explanation:</strong></p> <ul> <li><strong>Dot Product (QK^T):</strong> Measures similarity between queries and keys.</li> <li><strong>Scaling (dk\sqrt{d_k}dk​​):</strong> Prevents large dot product values leading to small gradients.</li> <li><strong>Softmax:</strong> Converts scores to probabilities representing attention weights.</li> <li><strong>Weighted Sum (V):</strong> Aggregates values based on attention weights to form contextualized embeddings.</li> </ul> <h3 id="43-scaled-dot-product-attention">4.3. Scaled Dot-Product Attention</h3> <p><strong>Formula:</strong></p> <p>Attention(Q,K,V)=softmax(QKTdk)V\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) VAttention(Q,K,V)=softmax(dk​​QKT​)V</p> <p><strong>Components Explained:</strong></p> <ul> <li><strong>Q (Query):</strong> Captures what we’re searching for.</li> <li><strong>K (Key):</strong> Captures what information we have.</li> <li><strong>V (Value):</strong> Contains the actual information.</li> </ul> <p><strong>Why Scale by dk\sqrt{d_k}dk​​?</strong></p> <ul> <li>To counteract the effect of high-dimensional dot products, which can result in extremely large values, leading to vanishing gradients during training.</li> </ul> <p><strong>Illustrative Diagram:</strong></p> <div class="language-css highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span><span class="nt">Input</span> <span class="nt">Embeddings</span><span class="o">]</span> <span class="nt">--</span><span class="o">&gt;</span> <span class="o">[</span><span class="nt">Q</span><span class="o">,</span> <span class="nt">K</span><span class="o">,</span> <span class="nt">V</span> <span class="nt">Linear</span> <span class="nt">Layers</span><span class="o">]</span> <span class="nt">--</span><span class="o">&gt;</span> <span class="o">[</span><span class="nt">Compute</span> <span class="nt">Attention</span> <span class="nt">Scores</span><span class="o">]</span> <span class="nt">--</span><span class="o">&gt;</span> <span class="o">[</span><span class="nt">Weighted</span> <span class="nt">Sum</span> <span class="nt">with</span> <span class="nt">V</span><span class="o">]</span> <span class="nt">--</span><span class="o">&gt;</span> <span class="o">[</span><span class="nt">Output</span> <span class="nt">Embeddings</span><span class="o">]</span>
</code></pre></div></div> <hr/> <h2 id="implementing-self-attention-in-code">Implementing Self-Attention in Code</h2> <hr/> <p>We’ll implement both single-head and multi-head self-attention mechanisms using PyTorch.</p> <h3 id="51-single-head-self-attention">5.1. Single-Head Self-Attention</h3> <p><strong>PyTorch Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_size</span> <span class="o">=</span> <span class="n">embed_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span>   <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: [batch_size, seq_length, embed_size]
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># [batch_size, seq_length, seq_length]
</span>        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_size</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># [batch_size, seq_length, embed_size]
</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p><strong>Usage Example:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
<span class="n">self_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">embed_size</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Output: torch.Size([2, 5, 64])`
</span></code></pre></div></div> <p><strong>Explanation:</strong></p> <ul> <li><strong>Input:</strong> Batch of sequences with embeddings.</li> <li><strong>Output:</strong> Contextualized embeddings where each position considers other positions in the sequence.</li> </ul> <h3 id="52-multi-head-self-attention">5.2. Multi-Head Self-Attention</h3> <p><strong>Why Multi-Head?</strong></p> <ul> <li>Allows the model to jointly attend to information from different representation subspaces at different positions.</li> </ul> <p><strong>PyTorch Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadSelfAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">embed_size</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">Embedding size must be divisible by num_heads</span><span class="sh">"</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embed_size</span> <span class="o">=</span> <span class="n">embed_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_size</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span>   <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">N</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Reshape for multi-head
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">embed_size</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p><strong>Usage Example:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
<span class="n">multi_head_attention</span> <span class="o">=</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">multi_head_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Output: torch.Size([2, 5, 64])
</span></code></pre></div></div> <p><strong>Explanation:</strong></p> <ul> <li><strong>Splitting into Heads:</strong> The embedding is divided into multiple heads, allowing the model to focus on different parts of the input.</li> <li><strong>Parallel Attention:</strong> Each head performs self-attention in parallel.</li> <li><strong>Concatenation and Projection:</strong> The outputs from all heads are concatenated and passed through a final linear layer.</li> </ul> <hr/> <h2 id="benefits-of-self-attention">Benefits of Self-Attention</h2> <hr/> <ul> <li><strong>Contextual Representations:</strong> Captures the context of words dynamically, handling polysemy effectively.</li> <li><strong>Parallelism:</strong> Unlike recurrent models, self-attention allows parallel computation over sequence positions, leading to faster training.</li> <li><strong>Long-Range Dependencies:</strong> Efficiently models relationships between distant words in a sequence.</li> <li><strong>Scalability:</strong> Performs well on long sequences without significant computational overhead.</li> <li><strong>Versatility:</strong> Applicable beyond NLP, including computer vision and speech processing tasks.</li> </ul> <p><strong>Comparison with Traditional Models:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>RNNs/LSTMs</th> <th>Self-Attention</th> </tr> </thead> <tbody> <tr> <td>Computation Parallelism</td> <td>Sequential</td> <td>Parallel</td> </tr> <tr> <td>Long-Range Dependencies</td> <td>Challenging to capture</td> <td>Efficiently captured</td> </tr> <tr> <td>Contextual Understanding</td> <td>Limited by sequence length</td> <td>Comprehensive across entire sequence</td> </tr> <tr> <td>Computational Efficiency</td> <td>Less efficient for long sequences</td> <td>More efficient and scalable</td> </tr> </tbody> </table> <hr/> <h2 id="conclusion">Conclusion</h2> <hr/> <p>Self-Attention has revolutionized the field of NLP by providing a powerful mechanism to capture contextual information effectively. It addresses the limitations of traditional word embeddings and recurrent models, enabling the development of sophisticated models like Transformers that have set new benchmarks across various tasks.</p> <p>Understanding self-attention is crucial for leveraging modern NLP architectures and for further innovation in the field.</p> <hr/> <h2 id="references">References</h2> <hr/> <ol> <li>Vaswani, A., et al. (2017). <a href="https://arxiv.org/abs/1706.03762"><strong>“Attention is All You Need”</strong></a>. <em>Advances in Neural Information Processing Systems</em>.</li> <li>Devlin, J., et al. (2018). <a href="https://arxiv.org/abs/1810.04805"><strong>“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”</strong></a>.</li> <li><strong>“The Illustrated Transformer”</strong> by Jay Alammar.</li> <li><strong>PyTorch Documentation</strong>.</li> </ol>]]></content><author><name></name></author><category term="transformers"/><category term=","/><category term="deeplearnings,"/><category term="sequence"/><category term="models"/><category term="self_attention"/><summary type="html"><![CDATA[understanding self-attension mechanism in transformers in details]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://farhanzafrani.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://farhanzafrani.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://farhanzafrani.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">In-Depth Look at the Multimodal Transformer for Unaligned Multimodal Language Sequences</title><link href="https://farhanzafrani.github.io/blog/2024/unaligned-multi-model/" rel="alternate" type="text/html" title="In-Depth Look at the Multimodal Transformer for Unaligned Multimodal Language Sequences"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://farhanzafrani.github.io/blog/2024/unaligned-multi-model</id><content type="html" xml:base="https://farhanzafrani.github.io/blog/2024/unaligned-multi-model/"><![CDATA[<p><img src="../assets/img/unaligned-multi-model-transformer.png" alt="title_image"/></p> <h4 id="introduction"><strong>Introduction</strong></h4> <p>The paper <strong>“Multimodal Transformer for Unaligned Multimodal Language Sequences”</strong> introduces a powerful approach to tackle unaligned multimodal sequences by leveraging the Transformer architecture. This is crucial for tasks like video understanding, sentiment analysis, and language grounding where different modalities, such as text, speech, and visual data, are not synchronized in time. In this post, we’ll dive deep into the <strong>model architecture</strong> and explain how the <strong>Multimodal Transformer (MT)</strong> achieves its performance with detailed explanations and code snippets to bring the concept to life.</p> <hr/> <h3 id="key-challenges-addressed-by-the-model"><strong>Key Challenges Addressed by the Model</strong></h3> <p>Before we dig into the architecture, let’s review the challenges the Multimodal Transformer aims to solve:</p> <ol> <li><strong>Unaligned Modalities</strong>: Different modalities (e.g., text and video) may not be temporally aligned.</li> <li><strong>Asynchronous Timing</strong>: The duration and timing of different modality streams can vary significantly.</li> <li><strong>Cross-Modal Dependencies</strong>: Capturing meaningful relationships between modalities, even when they are temporally misaligned. <hr/> <h3 id="multimodal-transformer-architecture"><strong>Multimodal Transformer Architecture</strong></h3> <p>The Multimodal Transformer (MT) is designed to handle input from multiple modalities and processes them simultaneously using a combination of <strong>self-attention</strong> and <strong>cross-modal attention</strong> mechanisms. The key components of the architecture are:</p> <h4 id="1-feature-extraction"><strong>1. Feature Extraction</strong></h4> <p>The first step is extracting features from each modality independently. These features are passed to modality-specific transformers for separate processing. Common feature extractors include:</p> <ul> <li><strong>Text</strong>: Using pre-trained embeddings like BERT or Word2Vec.</li> <li><strong>Audio</strong>: Using acoustic features like MFCC (Mel Frequency Cepstral Coefficients).</li> <li><strong>Visual</strong>: Using CNN-based feature extractors for image or video frames.</li> </ul> </li> </ol> <p>For simplicity, here’s a basic Python pseudocode for feature extraction:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">BertModel</span>
<span class="kn">import</span> <span class="n">librosa</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torchvision.models</span> <span class="k">as</span> <span class="n">models</span>

<span class="c1"># Text Feature Extraction
</span><span class="n">text_model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">)</span>
<span class="n">text_inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">sample text</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">pt</span><span class="sh">'</span><span class="p">)</span>
<span class="n">text_features</span> <span class="o">=</span> <span class="nf">text_model</span><span class="p">(</span><span class="o">**</span><span class="n">text_inputs</span><span class="p">).</span><span class="n">last_hidden_state</span>

<span class="c1"># Audio Feature Extraction
</span><span class="n">audio</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">audio_file.wav</span><span class="sh">'</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16000</span><span class="p">)</span>
<span class="n">audio_features</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="nf">mfcc</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">)</span>

<span class="c1"># Visual Feature Extraction
</span><span class="n">cnn_model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">visual_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>  <span class="c1"># Simulating an image
</span><span class="n">visual_features</span> <span class="o">=</span> <span class="nf">cnn_model</span><span class="p">(</span><span class="n">visual_input</span><span class="p">)</span>
</code></pre></div></div> <p>Each extracted feature set is passed to its respective <strong>modality-specific encoder</strong>.</p> <hr/> <h4 id="2-modality-specific-encoders"><strong>2. Modality-Specific Encoders</strong></h4> <p><img src="../assets/img/architecture.png" alt="Model Architecture"/> <em>image is taken from paper github repo</em></p> <p>Each modality (text, audio, video) is processed through its own transformer encoder, which consists of multiple layers of <strong>self-attention</strong> and <strong>feed-forward networks</strong>. These encoders learn the intra-modality relationships. The transformer encoder follows the traditional architecture:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">TransformerEncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="o">=</span><span class="n">nhead</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transformer_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer_encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="c1"># Example for Text
</span><span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">nhead</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">6</span>

<span class="n">text_encoder</span> <span class="o">=</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
<span class="n">text_encoded</span> <span class="o">=</span> <span class="nf">text_encoder</span><span class="p">(</span><span class="n">text_features</span><span class="p">)</span>
</code></pre></div></div> <p>Each modality’s encoder operates independently, generating <strong>modality-specific hidden representations</strong>.</p> <hr/> <h4 id="3-cross-modal-attention-mechanism"><strong>3. Cross-Modal Attention Mechanism</strong></h4> <p><img src="../assets/img/cross_model.png" alt="Cross Model Attention"/> <em>image is taken from paper github repo</em></p> <p>One of the most innovative parts of this model is the <strong>Cross-Modal Attention Mechanism</strong>, which allows different modalities to attend to each other. This mechanism is crucial for capturing relationships between text, audio, and visual streams, even when they are temporally misaligned.</p> <p>Cross-modal attention works by learning relationships between the representations of different modalities. The model computes <strong>attention weights</strong> between all elements across modalities, allowing the information flow between asynchronous data streams.</p> <p>Here’s a simplified code snippet demonstrating the cross-modal attention:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">CrossModalAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CrossModalAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">modality_a</span><span class="p">,</span> <span class="n">modality_b</span><span class="p">):</span>
        <span class="c1"># Q, K, V for cross-modality interaction
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">modality_a</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">modality_b</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">modality_b</span><span class="p">)</span>

        <span class="c1"># Attention computation
</span>        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">cross_modal_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">cross_modal_output</span>

<span class="c1"># Example: Text attending to Video features
</span><span class="n">cross_modal_attention</span> <span class="o">=</span> <span class="nc">CrossModalAttention</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">cross_modal_output</span> <span class="o">=</span> <span class="nf">cross_modal_attention</span><span class="p">(</span><span class="n">text_encoded</span><span class="p">,</span> <span class="n">visual_features</span><span class="p">)</span>
</code></pre></div></div> <p>In this example, we make <strong>text</strong> attend to <strong>video features</strong>, enabling the model to understand the connection between the text and corresponding visual actions, even if they aren’t perfectly aligned.</p> <hr/> <h4 id="4-multimodal-fusion-layer"><strong>4. Multimodal Fusion Layer</strong></h4> <p>Once cross-modal attention has been applied, the outputs from all modalities are combined into a unified representation. This <strong>fusion layer</strong> can be as simple as concatenation or more complex, using learned fusion techniques (e.g., weighted averaging or attention fusion). Here’s an example of multimodal fusion using concatenation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FusionLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">FusionLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text_features</span><span class="p">,</span> <span class="n">audio_features</span><span class="p">,</span> <span class="n">visual_features</span><span class="p">):</span>
        <span class="n">combined_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">text_features</span><span class="p">,</span> <span class="n">audio_features</span><span class="p">,</span> <span class="n">visual_features</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">fused_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">combined_features</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fused_output</span>

<span class="c1"># Fusion of text, audio, and video
</span><span class="n">fusion_layer</span> <span class="o">=</span> <span class="nc">FusionLayer</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">fused_representation</span> <span class="o">=</span> <span class="nf">fusion_layer</span><span class="p">(</span><span class="n">text_encoded</span><span class="p">,</span> <span class="n">audio_features</span><span class="p">,</span> <span class="n">visual_features</span><span class="p">)</span>
</code></pre></div></div> <p>In the example above, the output of each modality is concatenated, passed through a linear layer, and used to predict the final outcome (e.g., classification).</p> <hr/> <h4 id="5-final-prediction"><strong>5. Final Prediction</strong></h4> <p>The final step involves predicting the output (e.g., classification, sentiment score, action detection) from the fused multimodal representation. A fully connected layer (or layers) is applied to produce the final output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PredictionHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PredictionHead</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">fused_representation</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">fused_representation</span><span class="p">)</span>

<span class="c1"># Example: Sentiment Classification
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">prediction_head</span> <span class="o">=</span> <span class="nc">PredictionHead</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">final_output</span> <span class="o">=</span> <span class="nf">prediction_head</span><span class="p">(</span><span class="n">fused_representation</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h3 id="training-the-multimodal-transformer"><strong>Training the Multimodal Transformer</strong></h3> <p>The model is trained end-to-end using a <strong>contrastive loss</strong> that pushes similar representations of different modalities closer and dissimilar ones apart. The <strong>cross-modal contrastive loss</strong> helps in learning better interactions across asynchronous modalities. Here’s a brief code snippet to demonstrate how contrastive loss could be implemented:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ContrastiveLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ContrastiveLoss</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">pairwise_distance</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span>
               <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">label</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">margin</span> <span class="o">-</span> <span class="n">distance</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

<span class="c1"># Example: Compute contrastive loss between audio and visual representations
</span><span class="n">contrastive_loss</span> <span class="o">=</span> <span class="nc">ContrastiveLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">contrastive_loss</span><span class="p">(</span><span class="n">audio_features</span><span class="p">,</span> <span class="n">visual_features</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</code></pre></div></div> <p>This loss function helps the model learn meaningful relationships across modalities and improves its ability to generalize to unaligned sequences.</p> <hr/> <h3 id="conclusion"><strong>Conclusion</strong></h3> <p>The <strong>Multimodal Transformer for Unaligned Multimodal Language Sequences</strong> tackles a challenging problem in machine learning: integrating asynchronous and unaligned data from multiple modalities. By leveraging a combination of <strong>self-attention</strong>, <strong>cross-modal attention</strong>, and <strong>contrastive learning</strong>, this model successfully captures dependencies between different modalities, even when they’re not perfectly synchronized. The architecture, consisting of modality-specific transformers, cross-modal attention, and fusion layers, is both elegant and powerful, making it applicable across various tasks involving complex multimodal data. ***</p> <h3 id="references"><strong>References</strong></h3> <p><a href="https://github.com/yaohungt/Multimodal-Transformer">Paper Code</a></p> <p><a href="https://arxiv.org/pdf/1906.00295">Official Paper</a></p> <p><a href="https://www.youtube.com/watch?v=JTbX4OyUF-c">YouTube_video</a></p>]]></content><author><name></name></author><category term="transformers,"/><category term="deep-learning,"/><category term="multi-model-transformers"/><category term="multi-model-transformer"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://farhanzafrani.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://farhanzafrani.github.io/blog/2024/typograms</id><content type="html" xml:base="https://farhanzafrani.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://farhanzafrani.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://farhanzafrani.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://farhanzafrani.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://farhanzafrani.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://farhanzafrani.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://farhanzafrani.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">a post with code diff</title><link href="https://farhanzafrani.github.io/blog/2024/code-diff/" rel="alternate" type="text/html" title="a post with code diff"/><published>2024-01-27T19:22:00+00:00</published><updated>2024-01-27T19:22:00+00:00</updated><id>https://farhanzafrani.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://farhanzafrani.github.io/blog/2024/code-diff/"><![CDATA[<p>You can display diff code by using the regular markdown syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff
</span><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")
</span></code></pre></div></div> <p>But this is difficult to read, specially if you have a large diff. You can use <a href="https://diff2html.xyz/">diff2html</a> to display a more readable version of the diff. For this, just use <code class="language-plaintext highlighter-rouge">diff2html</code> instead of <code class="language-plaintext highlighter-rouge">diff</code> for the code block language:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff2html
</span><span class="sb">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
--- a/sample.js
+++ b/sample.js
@@ -1 +1 @@
-console.log("Hello World!")
+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>If we use a longer example, for example <a href="https://github.com/rtfpessoa/diff2html/commit/c2c253d3e3f8b8b267f551e659f72b44ca2ac927">this commit from diff2html</a>, it will generate the following output:</p> <pre><code class="language-diff2html">From 2aaae31cc2a37bfff83430c2c914b140bee59b6a Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sun, 9 Oct 2016 16:41:54 +0100
Subject: [PATCH 1/2] Initial template override support

---
 scripts/hulk.js                    |  4 ++--
 src/diff2html.js                   |  3 +--
 src/file-list-printer.js           | 11 ++++++++---
 src/hoganjs-utils.js               | 29 +++++++++++++++++------------
 src/html-printer.js                |  6 ++++++
 src/line-by-line-printer.js        |  6 +++++-
 src/side-by-side-printer.js        |  6 +++++-
 test/file-list-printer-tests.js    |  2 +-
 test/hogan-cache-tests.js          | 18 +++++++++++++++---
 test/line-by-line-tests.js         |  3 +--
 test/side-by-side-printer-tests.js |  3 +--
 11 files changed, 62 insertions(+), 29 deletions(-)

diff --git a/scripts/hulk.js b/scripts/hulk.js
index 5a793c18..a4b1a4d5 100755
--- a/scripts/hulk.js
+++ b/scripts/hulk.js
@@ -173,11 +173,11 @@ function namespace(name) {
 // write a template foreach file that matches template extension
 templates = extractFiles(options.argv.remain)
   .map(function(file) {
-    var openedFile = fs.readFileSync(file, 'utf-8');
+    var openedFile = fs.readFileSync(file, 'utf-8').trim();
     var name;
     if (!openedFile) return;
     name = namespace(path.basename(file).replace(/\..*$/, ''));
-    openedFile = removeByteOrderMark(openedFile.trim());
+    openedFile = removeByteOrderMark(openedFile);
     openedFile = wrap(file, name, openedFile);
     if (!options.outputdir) return openedFile;
     fs.writeFileSync(path.join(options.outputdir, name + '.js')
diff --git a/src/diff2html.js b/src/diff2html.js
index 21b0119e..64e138f5 100644
--- a/src/diff2html.js
+++ b/src/diff2html.js
@@ -7,7 +7,6 @@

 (function() {
   var diffParser = require('./diff-parser.js').DiffParser;
-  var fileLister = require('./file-list-printer.js').FileListPrinter;
   var htmlPrinter = require('./html-printer.js').HtmlPrinter;

   function Diff2Html() {
@@ -43,7 +42,7 @@

     var fileList = '';
     if (configOrEmpty.showFiles === true) {
-      fileList = fileLister.generateFileList(diffJson, configOrEmpty);
+      fileList = htmlPrinter.generateFileListSummary(diffJson, configOrEmpty);
     }

     var diffOutput = '';
diff --git a/src/file-list-printer.js b/src/file-list-printer.js
index e408d9b2..1e0a2c61 100644
--- a/src/file-list-printer.js
+++ b/src/file-list-printer.js
@@ -8,11 +8,16 @@
 (function() {
   var printerUtils = require('./printer-utils.js').PrinterUtils;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var baseTemplatesPath = 'file-summary';
   var iconsBaseTemplatesPath = 'icon';

-  function FileListPrinter() {
+  function FileListPrinter(config) {
+    this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   FileListPrinter.prototype.generateFileList = function(diffFiles) {
@@ -38,5 +43,5 @@
     });
   };

-  module.exports.FileListPrinter = new FileListPrinter();
+  module.exports.FileListPrinter = FileListPrinter;
 })();
diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 9949e5fa..0dda08d7 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -8,18 +8,19 @@
 (function() {
   var fs = require('fs');
   var path = require('path');
-
   var hogan = require('hogan.js');

   var hoganTemplates = require('./templates/diff2html-templates.js');

-  var templatesPath = path.resolve(__dirname, 'templates');
+  var extraTemplates;

-  function HoganJsUtils() {
+  function HoganJsUtils(configuration) {
+    this.config = configuration || {};
+    extraTemplates = this.config.templates || {};
   }

-  HoganJsUtils.prototype.render = function(namespace, view, params, configuration) {
-    var template = this.template(namespace, view, configuration);
+  HoganJsUtils.prototype.render = function(namespace, view, params) {
+    var template = this.template(namespace, view);
     if (template) {
       return template.render(params);
     }
@@ -27,17 +28,16 @@
     return null;
   };

-  HoganJsUtils.prototype.template = function(namespace, view, configuration) {
-    var config = configuration || {};
+  HoganJsUtils.prototype.template = function(namespace, view) {
     var templateKey = this._templateKey(namespace, view);

-    return this._getTemplate(templateKey, config);
+    return this._getTemplate(templateKey);
   };

-  HoganJsUtils.prototype._getTemplate = function(templateKey, config) {
+  HoganJsUtils.prototype._getTemplate = function(templateKey) {
     var template;

-    if (!config.noCache) {
+    if (!this.config.noCache) {
       template = this._readFromCache(templateKey);
     }

@@ -53,6 +53,7 @@

     try {
       if (fs.readFileSync) {
+        var templatesPath = path.resolve(__dirname, 'templates');
         var templatePath = path.join(templatesPath, templateKey);
         var templateContent = fs.readFileSync(templatePath + '.mustache', 'utf8');
         template = hogan.compile(templateContent);
@@ -66,12 +67,16 @@
   };

   HoganJsUtils.prototype._readFromCache = function(templateKey) {
-    return hoganTemplates[templateKey];
+    return extraTemplates[templateKey] || hoganTemplates[templateKey];
   };

   HoganJsUtils.prototype._templateKey = function(namespace, view) {
     return namespace + '-' + view;
   };

-  module.exports.HoganJsUtils = new HoganJsUtils();
+  HoganJsUtils.prototype.compile = function(templateStr) {
+    return hogan.compile(templateStr);
+  };
+
+  module.exports.HoganJsUtils = HoganJsUtils;
 })();
diff --git a/src/html-printer.js b/src/html-printer.js
index 585d5b66..13f83047 100644
--- a/src/html-printer.js
+++ b/src/html-printer.js
@@ -8,6 +8,7 @@
 (function() {
   var LineByLinePrinter = require('./line-by-line-printer.js').LineByLinePrinter;
   var SideBySidePrinter = require('./side-by-side-printer.js').SideBySidePrinter;
+  var FileListPrinter = require('./file-list-printer.js').FileListPrinter;

   function HtmlPrinter() {
   }
@@ -22,5 +23,10 @@
     return sideBySidePrinter.generateSideBySideJsonHtml(diffFiles);
   };

+  HtmlPrinter.prototype.generateFileListSummary = function(diffJson, config) {
+    var fileListPrinter = new FileListPrinter(config);
+    return fileListPrinter.generateFileList(diffJson);
+  };
+
   module.exports.HtmlPrinter = new HtmlPrinter();
 })();
diff --git a/src/line-by-line-printer.js b/src/line-by-line-printer.js
index b07eb53c..d230bedd 100644
--- a/src/line-by-line-printer.js
+++ b/src/line-by-line-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'line-by-line';
   var iconsBaseTemplatesPath = 'icon';
@@ -19,6 +20,9 @@

   function LineByLinePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   LineByLinePrinter.prototype.makeFileDiffHtml = function(file, diffs) {
diff --git a/src/side-by-side-printer.js b/src/side-by-side-printer.js
index bbf1dc8d..5e3033b3 100644
--- a/src/side-by-side-printer.js
+++ b/src/side-by-side-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'side-by-side';
   var iconsBaseTemplatesPath = 'icon';
@@ -26,6 +27,9 @@

   function SideBySidePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   SideBySidePrinter.prototype.makeDiffHtml = function(file, diffs) {
diff --git a/test/file-list-printer-tests.js b/test/file-list-printer-tests.js
index a502a46f..60ea3208 100644
--- a/test/file-list-printer-tests.js
+++ b/test/file-list-printer-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var fileListPrinter = require('../src/file-list-printer.js').FileListPrinter;
+var fileListPrinter = new (require('../src/file-list-printer.js').FileListPrinter)();

 describe('FileListPrinter', function() {
   describe('generateFileList', function() {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 190bf6f8..3bb754ac 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var HoganJsUtils = require('../src/hoganjs-utils.js').HoganJsUtils;
+var HoganJsUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)();
 var diffParser = require('../src/diff-parser.js').DiffParser;

 describe('HoganJsUtils', function() {
@@ -21,16 +21,28 @@ describe('HoganJsUtils', function() {
       });
       assert.equal(emptyDiffHtml, result);
     });
+
     it('should render view without cache', function() {
       var result = HoganJsUtils.render('generic', 'empty-diff', {
         contentClass: 'd2h-code-line',
         diffParser: diffParser
       }, {noCache: true});
-      assert.equal(emptyDiffHtml + '\n', result);
+      assert.equal(emptyDiffHtml, result);
     });
+
     it('should return null if template is missing', function() {
-      var result = HoganJsUtils.render('generic', 'missing-template', {}, {noCache: true});
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)({noCache: true});
+      var result = hoganUtils.render('generic', 'missing-template', {});
       assert.equal(null, result);
     });
+
+    it('should allow templates to be overridden', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+
+      var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
diff --git a/test/line-by-line-tests.js b/test/line-by-line-tests.js
index 1cd92073..8869b3df 100644
--- a/test/line-by-line-tests.js
+++ b/test/line-by-line-tests.js
@@ -14,7 +14,7 @@ describe('LineByLinePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expected, fileHtml);
     });
@@ -422,7 +422,6 @@ describe('LineByLinePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                &lt;/tbody&gt;\n' +
         '            &lt;/table&gt;\n' +
         '        &lt;/div&gt;\n' +
diff --git a/test/side-by-side-printer-tests.js b/test/side-by-side-printer-tests.js
index 76625f8e..771daaa5 100644
--- a/test/side-by-side-printer-tests.js
+++ b/test/side-by-side-printer-tests.js
@@ -14,7 +14,7 @@ describe('SideBySidePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expectedRight, fileHtml.right);
       assert.equal(expectedLeft, fileHtml.left);
@@ -324,7 +324,6 @@ describe('SideBySidePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                    &lt;/tbody&gt;\n' +
         '                &lt;/table&gt;\n' +
         '            &lt;/div&gt;\n' +

From f3cadb96677d0eb82fc2752dc3ffbf35ca9b5bdb Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sat, 15 Oct 2016 13:21:22 +0100
Subject: [PATCH 2/2] Allow uncompiled templates

---
 README.md                 |  3 +++
 src/hoganjs-utils.js      |  7 +++++++
 test/hogan-cache-tests.js | 24 +++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/README.md b/README.md
index 132c8a28..46909f25 100644
--- a/README.md
+++ b/README.md
@@ -98,6 +98,9 @@ The HTML output accepts a Javascript object with configuration. Possible options
   - `synchronisedScroll`: scroll both panes in side-by-side mode: `true` or `false`, default is `false`
   - `matchWordsThreshold`: similarity threshold for word matching, default is 0.25
   - `matchingMaxComparisons`: perform at most this much comparisons for line matching a block of changes, default is `2500`
+  - `templates`: object with previously compiled templates to replace parts of the html
+  - `rawTemplates`: object with raw not compiled templates to replace parts of the html
+  &gt; For more information regarding the possible templates look into [src/templates](https://github.com/rtfpessoa/diff2html/tree/master/src/templates)

 ## Diff2HtmlUI Helper

diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 0dda08d7..b2e9c275 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -17,6 +17,13 @@
   function HoganJsUtils(configuration) {
     this.config = configuration || {};
     extraTemplates = this.config.templates || {};
+
+    var rawTemplates = this.config.rawTemplates || {};
+    for (var templateName in rawTemplates) {
+      if (rawTemplates.hasOwnProperty(templateName)) {
+        if (!extraTemplates[templateName]) extraTemplates[templateName] = this.compile(rawTemplates[templateName]);
+      }
+    }
   }

   HoganJsUtils.prototype.render = function(namespace, view, params) {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 3bb754ac..a34839c0 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -36,7 +36,7 @@ describe('HoganJsUtils', function() {
       assert.equal(null, result);
     });

-    it('should allow templates to be overridden', function() {
+    it('should allow templates to be overridden with compiled templates', function() {
       var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');

       var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
@@ -44,5 +44,27 @@ describe('HoganJsUtils', function() {
       var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
       assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
     });
+
+    it('should allow templates to be overridden with uncompiled templates', function() {
+      var emptyDiffTemplate = '&lt;p&gt;&lt;/p&gt;';
+
+      var config = {rawTemplates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
+
+    it('should allow templates to be overridden giving priority to compiled templates', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+      var emptyDiffTemplateUncompiled = '&lt;p&gt;Not used!&lt;/p&gt;';
+
+      var config = {
+        templates: {'generic-empty-diff': emptyDiffTemplate},
+        rawTemplates: {'generic-empty-diff': emptyDiffTemplateUncompiled}
+      };
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is how you can display code diffs]]></summary></entry><entry><title type="html">a post with advanced image components</title><link href="https://farhanzafrani.github.io/blog/2024/advanced-images/" rel="alternate" type="text/html" title="a post with advanced image components"/><published>2024-01-27T11:46:00+00:00</published><updated>2024-01-27T11:46:00+00:00</updated><id>https://farhanzafrani.github.io/blog/2024/advanced-images</id><content type="html" xml:base="https://farhanzafrani.github.io/blog/2024/advanced-images/"><![CDATA[<p>This is an example post with advanced image components.</p> <h2 id="image-slider">Image Slider</h2> <p>This is a simple image slider. It uses the <a href="https://swiperjs.com/">Swiper</a> library. Check the <a href="https://swiperjs.com/demos">examples page</a> for more information of what you can achieve with it.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-480.webp 480w,/assets/img/10-800.webp 800w,/assets/img/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12-480.webp 480w,/assets/img/12-800.webp 800w,/assets/img/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h2 id="image-comparison-slider">Image Comparison Slider</h2> <p>This is a simple image comparison slider. It uses the <a href="https://img-comparison-slider.sneas.io/">img-comparison-slider</a> library. Check the <a href="https://img-comparison-slider.sneas.io/examples.html">examples page</a> for more information of what you can achieve with it.</p> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_color-480.webp 480w,/assets/img/prof_pic_color-800.webp 800w,/assets/img/prof_pic_color-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what advanced image components could look like]]></summary></entry><entry><title type="html">a post with vega lite</title><link href="https://farhanzafrani.github.io/blog/2024/vega-lite/" rel="alternate" type="text/html" title="a post with vega lite"/><published>2024-01-27T00:20:00+00:00</published><updated>2024-01-27T00:20:00+00:00</updated><id>https://farhanzafrani.github.io/blog/2024/vega-lite</id><content type="html" xml:base="https://farhanzafrani.github.io/blog/2024/vega-lite/"><![CDATA[<p>This is an example post with some <a href="https://vega.github.io/vega-lite/">vega lite</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">vega_lite
</span><span class="sb">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-vega_lite">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}
</code></pre> <p>This plot supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included vega lite code could look like]]></summary></entry></feed>